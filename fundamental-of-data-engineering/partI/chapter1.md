### Chapter 1. Data Engineering Described

- what is data engineering:
    - “Data engineering is a set of operations aimed at creating interfaces and mechanisms for the flow and access of information. It takes dedicated specialists—data engineers—to maintain data so that it remains available and usable by others. In short, data engineers set up and operate the organization’s data infrastructure, preparing it for further analysis by data analysts and scientists.”
    - “The first type of data engineering is SQL-focused. The work and primary storage of the data is in relational databases. All of the data processing is done with SQL or a SQL-based language. Sometimes, this data processing is done with an ETL tool. The second type of data engineering is Big Data–focused. The work and primary storage of the data is in Big Data technologies like Hadoop, Cassandra, and HBase. All of the data processing is done in Big Data frameworks like MapReduce, Spark, and Flink. While SQL is used, the primary processing is done with programming languages like Java, Scala, and Python.”
    - “In relation to previously existing roles, the data engineering field could be thought of as a superset of business intelligence and data warehousing that brings more elements from software engineering. This discipline also integrates specialization around the operation of so-called “big data” distributed systems, along with concepts around the extended Hadoop ecosystem, stream processing, and in computation at scale.”
    - “Data engineering is all about the movement, manipulation, and management of data.”
- “Data engineering is the development, implementation, and maintenance of systems and processes that take in raw data and produce high-quality, consistent information that supports downstream use cases, such as analysis and machine learning. Data engineering is the intersection of security, data management, DataOps, data architecture, orchestration, and software engineering. A data engineer manages the data engineering lifecycle, beginning with getting data from source systems and ending with serving data for use cases, such as analysis or machine learning.”
- MapReduce, an ultra-scalable data-processing paradigm
- With greater abstraction and simplification, a data lifecycle engineer
is no longer encumbered by the gory details of yesterday’s big data frameworks.
While data engineers maintain skills in low-level data programming and use these as
required, they increasingly find their role focused on things higher in the value chain:
**security, data management, DataOps, data architecture, orchestration, and general
data lifecycle management**. as they engineer pipelines,
they concern themselves with privacy, anonymization, data garbage collection, and
compliance with regulations.
- A data engineer gets data and provides value from the data, for furthur exploration by data scientist.
- a data engineer was expected to know and understand how to use a small handful of powerful and monolithic technologies (Hadoop,
Spark, Teradata, Hive, and many others) to create a data solution. Utilizing these
technologies often requires a sophisticated understanding of **software engineering,
networking, distributed computing, storage,** or other low-level details. Their work
would be devoted to cluster administration and maintenance, managing overhead,
and writing pipeline and transformation jobs, among other tasks.
- Hadoop and Spark are widely used for distributed data storage and parallel processing, while Kafka enables real-time data streaming across systems. NoSQL databases like Cassandra and MongoDB are designed to scale horizontally across many servers, handling large amounts of unstructured data efficiently.
- 
